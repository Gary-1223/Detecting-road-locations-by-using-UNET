{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "#刪除資料夾\n",
        "rm -rf /content/2"
      ],
      "metadata": {
        "id": "7wn3GpRUHfH0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6UDY9PW4Ll-5"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torchvision.transforms as transforms\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X0_MAIsVLny_"
      },
      "outputs": [],
      "source": [
        "# 定義卷積區塊 (ConvBlock)，用於 U-Net 編碼器和解碼器中\n",
        "class ConvBlock(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super().__init__()\n",
        "        # 兩層卷積+批量正規化+ReLU激活\n",
        "        self.conv = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, out_channels, 3, padding=1), # 第一個卷積層，保持輸出尺寸與輸入相同\n",
        "            nn.BatchNorm2d(out_channels),# 批量正規化，用於穩定訓練過程\n",
        "            nn.ReLU(inplace=True),# ReLU 激活函數，添加非線性特徵\n",
        "            nn.Conv2d(out_channels, out_channels, 3, padding=1),\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "            nn.ReLU(inplace=True)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "         # 前向傳播過程：將輸入 x 通過定義的卷積區塊\n",
        "        return self.conv(x)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kjBlgi23L3n2"
      },
      "outputs": [],
      "source": [
        "# 定義 U-Net 網絡結構\n",
        "class UNet(nn.Module):\n",
        "    def __init__(self, in_channels=3, out_channels=1):\n",
        "        super().__init__()\n",
        "\n",
        "        # 編碼器部分 (Encoder)\n",
        "        self.enc1 = ConvBlock(in_channels, 64)  # 第一層卷積區塊，輸入通道為圖像的通道數\n",
        "        self.enc2 = ConvBlock(64, 128)      # 第二層卷積區塊，將輸出通道增加到128\n",
        "        self.enc3 = ConvBlock(128, 256)     # 第三層卷積區塊\n",
        "        self.enc4 = ConvBlock(256, 512)     # 第四層卷積區塊\n",
        "\n",
        "        # Bottleneck (瓶頸部分)：對特徵圖進一步處理，提取最深層的特徵\n",
        "        self.bottleneck = ConvBlock(512, 512)  # 修改這裡，從1024改為512\n",
        "\n",
        "        # 解碼器部分 (Decoder)：通過反卷積和跳連接逐步恢復圖像尺寸\n",
        "        self.upconv4 = nn.ConvTranspose2d(512, 512, kernel_size=2, stride=2) #上采樣層，將特徵圖尺寸放大一倍\n",
        "        self.dec4 = ConvBlock(1024, 256)  # 512 + 512 = 1024 輸入通道\n",
        "\n",
        "        self.upconv3 = nn.ConvTranspose2d(256, 256, kernel_size=2, stride=2)\n",
        "        self.dec3 = ConvBlock(512, 128)   # 256 + 256 = 512 輸入通道\n",
        "\n",
        "        self.upconv2 = nn.ConvTranspose2d(128, 128, kernel_size=2, stride=2)\n",
        "        self.dec2 = ConvBlock(256, 64)    # 128 + 128 = 256 輸入通道\n",
        "\n",
        "        self.upconv1 = nn.ConvTranspose2d(64, 64, kernel_size=2, stride=2)\n",
        "        self.dec1 = ConvBlock(128, 64)    # 64 + 64 = 128 輸入通道\n",
        "\n",
        "        # 最終輸出層：將解碼器的輸出轉換為最終的分割結果\n",
        "        self.final_conv = nn.Conv2d(64, out_channels, 1)\n",
        "        # 池化層：用於編碼器部分，縮小特徵圖的尺寸\n",
        "        self.pool = nn.MaxPool2d(2)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # 編碼過程\n",
        "        enc1 = self.enc1(x)\n",
        "        enc2 = self.enc2(self.pool(enc1))\n",
        "        enc3 = self.enc3(self.pool(enc2))\n",
        "        enc4 = self.enc4(self.pool(enc3))\n",
        "\n",
        "        # Bottleneck：對編碼器最後一層的輸出進行進一步處理\n",
        "        bottleneck = self.bottleneck(self.pool(enc4))\n",
        "\n",
        "        # 解碼過程並添加跳連接\n",
        "        dec4 = self.upconv4(bottleneck)#上采樣\n",
        "        dec4 = torch.cat([dec4, enc4], dim=1)# 跳連接，將上采樣的輸出與編碼器對應層的輸出拼接\n",
        "        dec4 = self.dec4(dec4) # 卷積區塊\n",
        "\n",
        "        dec3 = self.upconv3(dec4)\n",
        "        dec3 = torch.cat([dec3, enc3], dim=1)\n",
        "        dec3 = self.dec3(dec3)\n",
        "\n",
        "        dec2 = self.upconv2(dec3)\n",
        "        dec2 = torch.cat([dec2, enc2], dim=1)\n",
        "        dec2 = self.dec2(dec2)\n",
        "\n",
        "        dec1 = self.upconv1(dec2)\n",
        "        dec1 = torch.cat([dec1, enc1], dim=1)\n",
        "        dec1 = self.dec1(dec1)\n",
        "\n",
        "        return self.final_conv(dec1)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import Model, Input\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Conv2DTranspose, concatenate\n",
        "from tensorflow.keras.utils import plot_model\n",
        "\n",
        "# 安裝必要套件\n",
        "import subprocess\n",
        "import sys\n",
        "\n",
        "def install_requirements():\n",
        "    packages = ['pydot', 'graphviz']\n",
        "    for package in packages:\n",
        "        subprocess.check_call([sys.executable, '-m', 'pip', 'install', package])\n",
        "\n",
        "def build_unet(input_shape=(256, 256, 3), out_channels=1):\n",
        "    inputs = Input(shape=input_shape)\n",
        "\n",
        "    # 編碼器部分 (Encoder)\n",
        "    enc1 = Conv2D(64, 3, activation='relu', padding='same')(inputs)\n",
        "    pool1 = MaxPooling2D(pool_size=(2, 2))(enc1)\n",
        "\n",
        "    enc2 = Conv2D(128, 3, activation='relu', padding='same')(pool1)\n",
        "    pool2 = MaxPooling2D(pool_size=(2, 2))(enc2)\n",
        "\n",
        "    enc3 = Conv2D(256, 3, activation='relu', padding='same')(pool2)\n",
        "    pool3 = MaxPooling2D(pool_size=(2, 2))(enc3)\n",
        "\n",
        "    enc4 = Conv2D(512, 3, activation='relu', padding='same')(pool3)\n",
        "    pool4 = MaxPooling2D(pool_size=(2, 2))(enc4)\n",
        "\n",
        "    # Bottleneck（瓶頸層）\n",
        "    bottleneck = Conv2D(512, 3, activation='relu', padding='same')(pool4)\n",
        "\n",
        "    # 解碼器部分 (Decoder)\n",
        "    up4 = Conv2DTranspose(512, 2, strides=(2, 2), padding='same')(bottleneck)\n",
        "    concat4 = concatenate([up4, enc4], axis=3)\n",
        "    dec4 = Conv2D(256, 3, activation='relu', padding='same')(concat4)\n",
        "\n",
        "    up3 = Conv2DTranspose(256, 2, strides=(2, 2), padding='same')(dec4)\n",
        "    concat3 = concatenate([up3, enc3], axis=3)\n",
        "    dec3 = Conv2D(128, 3, activation='relu', padding='same')(concat3)\n",
        "\n",
        "    up2 = Conv2DTranspose(128, 2, strides=(2, 2), padding='same')(dec3)\n",
        "    concat2 = concatenate([up2, enc2], axis=3)\n",
        "    dec2 = Conv2D(64, 3, activation='relu', padding='same')(concat2)\n",
        "\n",
        "    up1 = Conv2DTranspose(64, 2, strides=(2, 2), padding='same')(dec2)\n",
        "    concat1 = concatenate([up1, enc1], axis=3)\n",
        "    dec1 = Conv2D(64, 3, activation='relu', padding='same')(concat1)\n",
        "\n",
        "    # 最終輸出層\n",
        "    outputs = Conv2D(out_channels, 1, activation='sigmoid')(dec1)\n",
        "\n",
        "    model = Model(inputs=inputs, outputs=outputs)\n",
        "    return model\n",
        "\n",
        "def plot_unet_structure():\n",
        "    try:\n",
        "        # 安裝必要的套件\n",
        "        install_requirements()\n",
        "\n",
        "        # 創建模型\n",
        "        model = build_unet()\n",
        "\n",
        "        # 生成模型圖\n",
        "        plot_model(\n",
        "            model,\n",
        "            to_file='unet_structure.png',\n",
        "            show_shapes=True,\n",
        "            show_layer_names=True,\n",
        "            rankdir='TB',\n",
        "            dpi=96,\n",
        "            expand_nested=True\n",
        "        )\n",
        "        print(\"模型結構圖已保存為 'unet_structure.png'\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"生成圖形時出錯: {str(e)}\")\n",
        "        print(\"請確保已安裝 graphviz:\")\n",
        "        print(\"Ubuntu/Debian: sudo apt-get install graphviz\")\n",
        "        print(\"Mac: brew install graphviz\")\n",
        "        print(\"Windows: 從 graphviz.org 下載安裝包\")\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    plot_unet_structure()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ldkJVPYjuYjM",
        "outputId": "c7579abe-6f30-4c52-e9c0-935ac92654d2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "模型結構圖已保存為 'unet_structure.png'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yL1gFlumL6NT"
      },
      "outputs": [],
      "source": [
        "# 自定義資料集，用於加載圖像和遮罩\n",
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, image_dir, mask_dir, transform=None):\n",
        "        self.image_dir = image_dir\n",
        "        self.mask_dir = mask_dir\n",
        "        self.transform = transform\n",
        "\n",
        "        # 檢查圖像和遮罩資料夾是否存在\n",
        "        if not os.path.exists(image_dir):\n",
        "            raise ValueError(f\"圖像目錄不存在: {image_dir}\")\n",
        "        if not os.path.exists(mask_dir):\n",
        "            raise ValueError(f\"遮罩目錄不存在: {mask_dir}\")\n",
        "\n",
        "        self.images = [f for f in os.listdir(image_dir) if f.endswith('.png')]\n",
        "        self.valid_pairs = []\n",
        "\n",
        "        # 檢查每個圖像對應的遮罩是否存在\n",
        "        for img_name in self.images:\n",
        "            base_name = img_name.rsplit('.', 1)[0]\n",
        "            mask_name = f\"{base_name}_labeled.png\"#標籤檔名\n",
        "            mask_path = os.path.join(mask_dir, mask_name)\n",
        "\n",
        "            if os.path.exists(mask_path):\n",
        "                self.valid_pairs.append((img_name, mask_name))\n",
        "                print(f\"找到有效配對 - 圖像: {img_name}, 遮罩: {mask_name}\")\n",
        "\n",
        "        if not self.valid_pairs:\n",
        "            raise ValueError(\"找不到有效的圖像-遮罩配對\")\n",
        "\n",
        "        print(f\"找到 {len(self.valid_pairs)} 個有效的圖像-遮罩配對\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.valid_pairs)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        try:\n",
        "            img_name, mask_name = self.valid_pairs[idx]\n",
        "            image_path = os.path.join(self.image_dir, img_name)\n",
        "            mask_path = os.path.join(self.mask_dir, mask_name)\n",
        "\n",
        "            # 打開圖像和遮罩\n",
        "            image = Image.open(image_path).convert('RGB')\n",
        "            mask = Image.open(mask_path).convert('L')\n",
        "\n",
        "            # 應用變換\n",
        "            if self.transform:\n",
        "                image = self.transform['image'](image)\n",
        "                mask = self.transform['mask'](mask)\n",
        "\n",
        "            return image, mask\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"加載圖像對 {img_name} 時出錯: {str(e)}\")\n",
        "            raise"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_accuracy(outputs, masks, threshold=0.5):\n",
        "    \"\"\"計算分割準確度\"\"\"\n",
        "    with torch.no_grad():\n",
        "        # 將輸出轉換為二值預測\n",
        "        predicted = torch.sigmoid(outputs) > threshold\n",
        "        masks = masks > threshold\n",
        "\n",
        "        # 計算IoU (Intersection over Union)\n",
        "        intersection = (predicted & masks).float().sum((1, 2, 3))#  Intersection (交集)：預測分割區域與真實分割區域重疊的部分。\n",
        "        union = (predicted | masks).float().sum((1, 2, 3))#Union (聯集)：預測分割區域和真實分割區域的總和，不重複計算重疊的部分。\n",
        "        iou = (intersection + 1e-6) / (union + 1e-6)\n",
        "\n",
        "        # 計算像素準確度:比較預測結果與真實標籤，計算每個像素的準確性，然後取平均\n",
        "        pixel_accuracy = (predicted == masks).float().mean((1, 2, 3))\n",
        "\n",
        "        return iou.mean().item(), pixel_accuracy.mean().item()\n"
      ],
      "metadata": {
        "id": "GMfmFyUuj0uL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(model, train_loader, criterion, optimizer, device, num_epochs=100):\n",
        "    model.train()# 設置模型為訓練模式\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        running_loss = 0.0 # 初始化每個 epoch 的累積損失\n",
        "\n",
        "        for batch_idx, (images, masks) in enumerate(train_loader):\n",
        "            images = images.to(device)\n",
        "            masks = masks.to(device)\n",
        "\n",
        "            optimizer.zero_grad()# 清除上一次梯度\n",
        "            outputs = model(images)  # 將圖像輸入模型獲取輸出\n",
        "            loss = criterion(outputs, masks) # 計算損失\n",
        "            loss.backward()# 反向傳播計算梯度\n",
        "            optimizer.step()# 更新模型參數\n",
        "\n",
        "            running_loss += loss.item()# 累加損失\n",
        "\n",
        "            if batch_idx % 3 == 0:\n",
        "                print(f'Epoch [{epoch+1}/{num_epochs}], Batch [{batch_idx}/{len(train_loader)}], Loss: {loss.item():.4f}')\n",
        "        # 計算並打印該 epoch 的平均損失\n",
        "        epoch_loss = running_loss / len(train_loader)\n",
        "        print(f'Epoch [{epoch+1}/{num_epochs}], Average Loss: {epoch_loss:.4f}')\n",
        "\n",
        "        # 每2個epoch計算並輸出準確度\n",
        "        if (epoch + 1) % 2 == 0:\n",
        "            model.eval()# 設置模型為評估模式\n",
        "            total_iou = 0.0\n",
        "            total_pixel_acc = 0.0\n",
        "            num_batches = 0\n",
        "\n",
        "            with torch.no_grad():# 關閉梯度計算，節省記憶體(在評估階段不需要計算梯度)\n",
        "                for images, masks in train_loader:\n",
        "                    images = images.to(device)\n",
        "                    masks = masks.to(device)\n",
        "                    outputs = model(images)\n",
        "\n",
        "                    iou, pixel_acc = calculate_accuracy(outputs, masks)\n",
        "                    total_iou += iou\n",
        "                    total_pixel_acc += pixel_acc\n",
        "                    num_batches += 1\n",
        "\n",
        "            # 計算平均 IoU 和像素準確度\n",
        "            avg_iou = total_iou / num_batches\n",
        "            avg_pixel_acc = total_pixel_acc / num_batches\n",
        "            print(f'Epoch [{epoch+1}/{num_epochs}] Metrics:')\n",
        "            print(f'Average IoU: {avg_iou:.4f}')\n",
        "            print(f'Average Pixel Accuracy: {avg_pixel_acc:.4f}')\n",
        "\n",
        "            model.train()# 恢復模型到訓練模式"
      ],
      "metadata": {
        "id": "ZQZVWK__jPnY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def main():\n",
        "    # 設置設備為 GPU (cuda) 如果可用，否則使用 CPU\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    print(f\"使用設備: {device}\")\n",
        "\n",
        "    image_dir = \"/content/1\"\n",
        "    mask_dir = \"/content/2\"\n",
        "\n",
        "    model = UNet(in_channels=3, out_channels=1).to(device)\n",
        "    # 設置損失函數為二元交叉熵損失，適用於二分類問題\n",
        "    criterion = nn.BCEWithLogitsLoss()\n",
        "    # 使用 Adam 優化器，學習率為 1e-4\n",
        "    optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
        "\n",
        "    transforms_dict = {\n",
        "        'image': transforms.Compose([\n",
        "            transforms.Resize((256, 256)), # 將圖像調整到 256x256\n",
        "            transforms.ToTensor(),# 將圖像轉換為張量\n",
        "            transforms.Normalize(mean=[0.485, 0.456, 0.406], # 正規化圖像\n",
        "                               std=[0.229, 0.224, 0.225])\n",
        "        ]),\n",
        "        'mask': transforms.Compose([\n",
        "            transforms.Resize((256, 256)),\n",
        "            transforms.ToTensor()\n",
        "        ])\n",
        "    }\n",
        "\n",
        "    try:\n",
        "        dataset = CustomDataset(image_dir, mask_dir, transform=transforms_dict)\n",
        "        train_loader = DataLoader(dataset, batch_size=4, shuffle=True)\n",
        "\n",
        "        print(\"開始訓練...\")\n",
        "        train_model(model, train_loader, criterion, optimizer, device, num_epochs=20)\n",
        "\n",
        "        torch.save(model.state_dict(), 'unet_model.pth')\n",
        "        print(\"模型已保存為 'unet_model.pth'\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"程序執行出錯: {str(e)}\")\n",
        "        raise\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kV_G0Unaj9uy",
        "outputId": "009ab7a6-f8d9-499c-acd3-587552c7cf99"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "使用設備: cuda\n",
            "找到有效配對 - 圖像: 0006R0_f02130.png, 遮罩: 0006R0_f02130_labeled.png\n",
            "找到有效配對 - 圖像: 0001TP_010350.png, 遮罩: 0001TP_010350_labeled.png\n",
            "找到有效配對 - 圖像: 0006R0_f01530.png, 遮罩: 0006R0_f01530_labeled.png\n",
            "找到有效配對 - 圖像: 0006R0_f02460.png, 遮罩: 0006R0_f02460_labeled.png\n",
            "找到有效配對 - 圖像: 0006R0_f00930.png, 遮罩: 0006R0_f00930_labeled.png\n",
            "找到有效配對 - 圖像: 0006R0_f01800.png, 遮罩: 0006R0_f01800_labeled.png\n",
            "找到有效配對 - 圖像: 0001TP_010320.png, 遮罩: 0001TP_010320_labeled.png\n",
            "找到有效配對 - 圖像: 0006R0_f02730.png, 遮罩: 0006R0_f02730_labeled.png\n",
            "找到有效配對 - 圖像: 0006R0_f02010.png, 遮罩: 0006R0_f02010_labeled.png\n",
            "找到有效配對 - 圖像: 0006R0_f01830.png, 遮罩: 0006R0_f01830_labeled.png\n",
            "找到有效配對 - 圖像: 0001TP_010020.png, 遮罩: 0001TP_010020_labeled.png\n",
            "找到有效配對 - 圖像: 0001TP_009450.png, 遮罩: 0001TP_009450_labeled.png\n",
            "找到有效配對 - 圖像: 0006R0_f01470.png, 遮罩: 0006R0_f01470_labeled.png\n",
            "找到有效配對 - 圖像: 0006R0_f01860.png, 遮罩: 0006R0_f01860_labeled.png\n",
            "找到有效配對 - 圖像: 0006R0_f02640.png, 遮罩: 0006R0_f02640_labeled.png\n",
            "找到有效配對 - 圖像: 0006R0_f01230.png, 遮罩: 0006R0_f01230_labeled.png\n",
            "找到有效配對 - 圖像: 0006R0_f01110.png, 遮罩: 0006R0_f01110_labeled.png\n",
            "找到有效配對 - 圖像: 0001TP_009240.png, 遮罩: 0001TP_009240_labeled.png\n",
            "找到有效配對 - 圖像: 0001TP_010110.png, 遮罩: 0001TP_010110_labeled.png\n",
            "找到有效配對 - 圖像: 0001TP_009210.png, 遮罩: 0001TP_009210_labeled.png\n",
            "找到有效配對 - 圖像: 0006R0_f01170.png, 遮罩: 0006R0_f01170_labeled.png\n",
            "找到有效配對 - 圖像: 0006R0_f01140.png, 遮罩: 0006R0_f01140_labeled.png\n",
            "找到有效配對 - 圖像: 0006R0_f01770.png, 遮罩: 0006R0_f01770_labeled.png\n",
            "找到有效配對 - 圖像: 0001TP_010290.png, 遮罩: 0001TP_010290_labeled.png\n",
            "找到有效配對 - 圖像: 0001TP_010140.png, 遮罩: 0001TP_010140_labeled.png\n",
            "找到有效配對 - 圖像: 0001TP_009750.png, 遮罩: 0001TP_009750_labeled.png\n",
            "找到有效配對 - 圖像: 0006R0_f02340.png, 遮罩: 0006R0_f02340_labeled.png\n",
            "找到有效配對 - 圖像: 0001TP_010380.png, 遮罩: 0001TP_010380_labeled.png\n",
            "找到有效配對 - 圖像: 0006R0_f02040.png, 遮罩: 0006R0_f02040_labeled.png\n",
            "找到有效配對 - 圖像: 0001TP_009420.png, 遮罩: 0001TP_009420_labeled.png\n",
            "找到有效配對 - 圖像: 0001TP_009810.png, 遮罩: 0001TP_009810_labeled.png\n",
            "找到有效配對 - 圖像: 0001TP_009510.png, 遮罩: 0001TP_009510_labeled.png\n",
            "找到有效配對 - 圖像: 0006R0_f02070.png, 遮罩: 0006R0_f02070_labeled.png\n",
            "找到有效配對 - 圖像: 0006R0_f00960.png, 遮罩: 0006R0_f00960_labeled.png\n",
            "找到有效配對 - 圖像: 0006R0_f01260.png, 遮罩: 0006R0_f01260_labeled.png\n",
            "找到有效配對 - 圖像: 0001TP_009540.png, 遮罩: 0001TP_009540_labeled.png\n",
            "找到有效配對 - 圖像: 0006R0_f01560.png, 遮罩: 0006R0_f01560_labeled.png\n",
            "找到有效配對 - 圖像: 0006R0_f01200.png, 遮罩: 0006R0_f01200_labeled.png\n",
            "找到有效配對 - 圖像: 0001TP_009480.png, 遮罩: 0001TP_009480_labeled.png\n",
            "找到有效配對 - 圖像: 0006R0_f02160.png, 遮罩: 0006R0_f02160_labeled.png\n",
            "找到有效配對 - 圖像: 0001TP_009840.png, 遮罩: 0001TP_009840_labeled.png\n",
            "找到有效配對 - 圖像: 0006R0_f02430.png, 遮罩: 0006R0_f02430_labeled.png\n",
            "找到有效配對 - 圖像: 0006R0_f02760.png, 遮罩: 0006R0_f02760_labeled.png\n",
            "找到有效配對 - 圖像: 0006R0_f02100.png, 遮罩: 0006R0_f02100_labeled.png\n",
            "找到有效配對 - 圖像: 0006R0_f02370.png, 遮罩: 0006R0_f02370_labeled.png\n",
            "找到有效配對 - 圖像: 0006R0_f02610.png, 遮罩: 0006R0_f02610_labeled.png\n",
            "找到有效配對 - 圖像: 0006R0_f02310.png, 遮罩: 0006R0_f02310_labeled.png\n",
            "找到有效配對 - 圖像: 0001TP_009990.png, 遮罩: 0001TP_009990_labeled.png\n",
            "找到有效配對 - 圖像: 0006R0_f02940.png, 遮罩: 0006R0_f02940_labeled.png\n",
            "找到有效配對 - 圖像: 0006R0_f02670.png, 遮罩: 0006R0_f02670_labeled.png\n",
            "找到有效配對 - 圖像: 0006R0_f01500.png, 遮罩: 0006R0_f01500_labeled.png\n",
            "找到有效配對 - 圖像: 0001TP_009690.png, 遮罩: 0001TP_009690_labeled.png\n",
            "找到有效配對 - 圖像: 0001TP_010080.png, 遮罩: 0001TP_010080_labeled.png\n",
            "找到有效配對 - 圖像: 0006R0_f01710.png, 遮罩: 0006R0_f01710_labeled.png\n",
            "找到有效配對 - 圖像: 0006R0_f01740.png, 遮罩: 0006R0_f01740_labeled.png\n",
            "找到有效配對 - 圖像: 0006R0_f02910.png, 遮罩: 0006R0_f02910_labeled.png\n",
            "找到有效配對 - 圖像: 0006R0_f01440.png, 遮罩: 0006R0_f01440_labeled.png\n",
            "找到有效配對 - 圖像: 0006R0_f02400.png, 遮罩: 0006R0_f02400_labeled.png\n",
            "找到有效配對 - 圖像: 0001TP_010050.png, 遮罩: 0001TP_010050_labeled.png\n",
            "找到有效配對 - 圖像: 0001TP_009780.png, 遮罩: 0001TP_009780_labeled.png\n",
            "找到有效配對 - 圖像: 0006R0_f02700.png, 遮罩: 0006R0_f02700_labeled.png\n",
            "找到有效配對 - 圖像: 0001TP_009390.png, 遮罩: 0001TP_009390_labeled.png\n",
            "找到有效配對 - 圖像: 0006R0_f01410.png, 遮罩: 0006R0_f01410_labeled.png\n",
            "找到有效配對 - 圖像: 0001TP_009720.png, 遮罩: 0001TP_009720_labeled.png\n",
            "找到 64 個有效的圖像-遮罩配對\n",
            "開始訓練...\n",
            "Epoch [1/20], Batch [0/16], Loss: 0.6706\n",
            "Epoch [1/20], Batch [3/16], Loss: 0.5452\n",
            "Epoch [1/20], Batch [6/16], Loss: 0.4601\n",
            "Epoch [1/20], Batch [9/16], Loss: 0.4976\n",
            "Epoch [1/20], Batch [12/16], Loss: 0.3636\n",
            "Epoch [1/20], Batch [15/16], Loss: 0.3834\n",
            "Epoch [1/20], Average Loss: 0.4773\n",
            "Epoch [2/20], Batch [0/16], Loss: 0.5401\n",
            "Epoch [2/20], Batch [3/16], Loss: 0.4282\n",
            "Epoch [2/20], Batch [6/16], Loss: 0.3181\n",
            "Epoch [2/20], Batch [9/16], Loss: 0.3328\n",
            "Epoch [2/20], Batch [12/16], Loss: 0.2534\n",
            "Epoch [2/20], Batch [15/16], Loss: 0.2469\n",
            "Epoch [2/20], Average Loss: 0.3284\n",
            "Epoch [2/20] Metrics:\n",
            "Average IoU: 0.4934\n",
            "Average Pixel Accuracy: 0.6266\n",
            "Epoch [3/20], Batch [0/16], Loss: 0.2323\n",
            "Epoch [3/20], Batch [3/16], Loss: 0.2604\n",
            "Epoch [3/20], Batch [6/16], Loss: 0.3243\n",
            "Epoch [3/20], Batch [9/16], Loss: 0.3132\n",
            "Epoch [3/20], Batch [12/16], Loss: 0.3100\n",
            "Epoch [3/20], Batch [15/16], Loss: 0.2163\n",
            "Epoch [3/20], Average Loss: 0.2731\n",
            "Epoch [4/20], Batch [0/16], Loss: 0.3205\n",
            "Epoch [4/20], Batch [3/16], Loss: 0.2069\n",
            "Epoch [4/20], Batch [6/16], Loss: 0.2731\n",
            "Epoch [4/20], Batch [9/16], Loss: 0.2169\n",
            "Epoch [4/20], Batch [12/16], Loss: 0.2061\n",
            "Epoch [4/20], Batch [15/16], Loss: 0.2199\n",
            "Epoch [4/20], Average Loss: 0.2336\n",
            "Epoch [4/20] Metrics:\n",
            "Average IoU: 0.9226\n",
            "Average Pixel Accuracy: 0.9428\n",
            "Epoch [5/20], Batch [0/16], Loss: 0.2052\n",
            "Epoch [5/20], Batch [3/16], Loss: 0.2041\n",
            "Epoch [5/20], Batch [6/16], Loss: 0.2108\n",
            "Epoch [5/20], Batch [9/16], Loss: 0.1877\n",
            "Epoch [5/20], Batch [12/16], Loss: 0.2295\n",
            "Epoch [5/20], Batch [15/16], Loss: 0.1711\n",
            "Epoch [5/20], Average Loss: 0.2179\n",
            "Epoch [6/20], Batch [0/16], Loss: 0.1650\n",
            "Epoch [6/20], Batch [3/16], Loss: 0.2928\n",
            "Epoch [6/20], Batch [6/16], Loss: 0.1979\n",
            "Epoch [6/20], Batch [9/16], Loss: 0.1952\n",
            "Epoch [6/20], Batch [12/16], Loss: 0.2614\n",
            "Epoch [6/20], Batch [15/16], Loss: 0.1674\n",
            "Epoch [6/20], Average Loss: 0.2175\n",
            "Epoch [6/20] Metrics:\n",
            "Average IoU: 0.9287\n",
            "Average Pixel Accuracy: 0.9463\n",
            "Epoch [7/20], Batch [0/16], Loss: 0.2346\n",
            "Epoch [7/20], Batch [3/16], Loss: 0.1913\n",
            "Epoch [7/20], Batch [6/16], Loss: 0.2291\n",
            "Epoch [7/20], Batch [9/16], Loss: 0.2112\n",
            "Epoch [7/20], Batch [12/16], Loss: 0.1538\n",
            "Epoch [7/20], Batch [15/16], Loss: 0.2236\n",
            "Epoch [7/20], Average Loss: 0.2134\n",
            "Epoch [8/20], Batch [0/16], Loss: 0.2183\n",
            "Epoch [8/20], Batch [3/16], Loss: 0.1881\n",
            "Epoch [8/20], Batch [6/16], Loss: 0.1737\n",
            "Epoch [8/20], Batch [9/16], Loss: 0.1692\n",
            "Epoch [8/20], Batch [12/16], Loss: 0.2550\n",
            "Epoch [8/20], Batch [15/16], Loss: 0.1719\n",
            "Epoch [8/20], Average Loss: 0.1978\n",
            "Epoch [8/20] Metrics:\n",
            "Average IoU: 0.9405\n",
            "Average Pixel Accuracy: 0.9550\n",
            "Epoch [9/20], Batch [0/16], Loss: 0.1637\n",
            "Epoch [9/20], Batch [3/16], Loss: 0.1927\n",
            "Epoch [9/20], Batch [6/16], Loss: 0.2243\n",
            "Epoch [9/20], Batch [9/16], Loss: 0.2231\n",
            "Epoch [9/20], Batch [12/16], Loss: 0.1908\n",
            "Epoch [9/20], Batch [15/16], Loss: 0.1740\n",
            "Epoch [9/20], Average Loss: 0.1858\n",
            "Epoch [10/20], Batch [0/16], Loss: 0.1815\n",
            "Epoch [10/20], Batch [3/16], Loss: 0.1680\n",
            "Epoch [10/20], Batch [6/16], Loss: 0.1585\n",
            "Epoch [10/20], Batch [9/16], Loss: 0.1923\n",
            "Epoch [10/20], Batch [12/16], Loss: 0.1629\n",
            "Epoch [10/20], Batch [15/16], Loss: 0.1413\n",
            "Epoch [10/20], Average Loss: 0.1798\n",
            "Epoch [10/20] Metrics:\n",
            "Average IoU: 0.9479\n",
            "Average Pixel Accuracy: 0.9617\n",
            "Epoch [11/20], Batch [0/16], Loss: 0.1985\n",
            "Epoch [11/20], Batch [3/16], Loss: 0.1683\n",
            "Epoch [11/20], Batch [6/16], Loss: 0.1579\n",
            "Epoch [11/20], Batch [9/16], Loss: 0.1920\n",
            "Epoch [11/20], Batch [12/16], Loss: 0.2194\n",
            "Epoch [11/20], Batch [15/16], Loss: 0.1735\n",
            "Epoch [11/20], Average Loss: 0.1694\n",
            "Epoch [12/20], Batch [0/16], Loss: 0.1367\n",
            "Epoch [12/20], Batch [3/16], Loss: 0.1463\n",
            "Epoch [12/20], Batch [6/16], Loss: 0.1446\n",
            "Epoch [12/20], Batch [9/16], Loss: 0.1652\n",
            "Epoch [12/20], Batch [12/16], Loss: 0.1650\n",
            "Epoch [12/20], Batch [15/16], Loss: 0.1333\n",
            "Epoch [12/20], Average Loss: 0.1634\n",
            "Epoch [12/20] Metrics:\n",
            "Average IoU: 0.9391\n",
            "Average Pixel Accuracy: 0.9539\n",
            "Epoch [13/20], Batch [0/16], Loss: 0.1382\n",
            "Epoch [13/20], Batch [3/16], Loss: 0.2284\n",
            "Epoch [13/20], Batch [6/16], Loss: 0.1912\n",
            "Epoch [13/20], Batch [9/16], Loss: 0.1418\n",
            "Epoch [13/20], Batch [12/16], Loss: 0.1400\n",
            "Epoch [13/20], Batch [15/16], Loss: 0.1842\n",
            "Epoch [13/20], Average Loss: 0.1689\n",
            "Epoch [14/20], Batch [0/16], Loss: 0.1366\n",
            "Epoch [14/20], Batch [3/16], Loss: 0.1778\n",
            "Epoch [14/20], Batch [6/16], Loss: 0.1493\n",
            "Epoch [14/20], Batch [9/16], Loss: 0.1845\n",
            "Epoch [14/20], Batch [12/16], Loss: 0.1419\n",
            "Epoch [14/20], Batch [15/16], Loss: 0.1434\n",
            "Epoch [14/20], Average Loss: 0.1511\n",
            "Epoch [14/20] Metrics:\n",
            "Average IoU: 0.9549\n",
            "Average Pixel Accuracy: 0.9662\n",
            "Epoch [15/20], Batch [0/16], Loss: 0.1260\n",
            "Epoch [15/20], Batch [3/16], Loss: 0.1493\n",
            "Epoch [15/20], Batch [6/16], Loss: 0.1338\n",
            "Epoch [15/20], Batch [9/16], Loss: 0.1652\n",
            "Epoch [15/20], Batch [12/16], Loss: 0.1645\n",
            "Epoch [15/20], Batch [15/16], Loss: 0.1753\n",
            "Epoch [15/20], Average Loss: 0.1441\n",
            "Epoch [16/20], Batch [0/16], Loss: 0.1509\n",
            "Epoch [16/20], Batch [3/16], Loss: 0.1197\n",
            "Epoch [16/20], Batch [6/16], Loss: 0.1585\n",
            "Epoch [16/20], Batch [9/16], Loss: 0.1278\n",
            "Epoch [16/20], Batch [12/16], Loss: 0.1527\n",
            "Epoch [16/20], Batch [15/16], Loss: 0.1978\n",
            "Epoch [16/20], Average Loss: 0.1354\n",
            "Epoch [16/20] Metrics:\n",
            "Average IoU: 0.9623\n",
            "Average Pixel Accuracy: 0.9720\n",
            "Epoch [17/20], Batch [0/16], Loss: 0.1565\n",
            "Epoch [17/20], Batch [3/16], Loss: 0.1614\n",
            "Epoch [17/20], Batch [6/16], Loss: 0.1289\n",
            "Epoch [17/20], Batch [9/16], Loss: 0.1094\n",
            "Epoch [17/20], Batch [12/16], Loss: 0.1581\n",
            "Epoch [17/20], Batch [15/16], Loss: 0.1485\n",
            "Epoch [17/20], Average Loss: 0.1439\n",
            "Epoch [18/20], Batch [0/16], Loss: 0.1423\n",
            "Epoch [18/20], Batch [3/16], Loss: 0.1799\n",
            "Epoch [18/20], Batch [6/16], Loss: 0.1115\n",
            "Epoch [18/20], Batch [9/16], Loss: 0.1182\n",
            "Epoch [18/20], Batch [12/16], Loss: 0.1271\n",
            "Epoch [18/20], Batch [15/16], Loss: 0.1220\n",
            "Epoch [18/20], Average Loss: 0.1400\n",
            "Epoch [18/20] Metrics:\n",
            "Average IoU: 0.9511\n",
            "Average Pixel Accuracy: 0.9636\n",
            "Epoch [19/20], Batch [0/16], Loss: 0.1551\n",
            "Epoch [19/20], Batch [3/16], Loss: 0.1226\n",
            "Epoch [19/20], Batch [6/16], Loss: 0.1154\n",
            "Epoch [19/20], Batch [9/16], Loss: 0.1367\n",
            "Epoch [19/20], Batch [12/16], Loss: 0.1351\n",
            "Epoch [19/20], Batch [15/16], Loss: 0.1263\n",
            "Epoch [19/20], Average Loss: 0.1316\n",
            "Epoch [20/20], Batch [0/16], Loss: 0.1224\n",
            "Epoch [20/20], Batch [3/16], Loss: 0.1110\n",
            "Epoch [20/20], Batch [6/16], Loss: 0.1221\n",
            "Epoch [20/20], Batch [9/16], Loss: 0.0969\n",
            "Epoch [20/20], Batch [12/16], Loss: 0.1269\n",
            "Epoch [20/20], Batch [15/16], Loss: 0.1275\n",
            "Epoch [20/20], Average Loss: 0.1187\n",
            "Epoch [20/20] Metrics:\n",
            "Average IoU: 0.9706\n",
            "Average Pixel Accuracy: 0.9786\n",
            "模型已保存為 'unet_model.pth'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2cGIp15NNlDS",
        "outputId": "4fb58922-1379-4f66-b7ea-8eaf93c1fa05"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-10-df6264d38083>:16: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  self.model.load_state_dict(torch.load(model_path, map_location=self.device))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "開始處理 64 張圖片...\n",
            "處理圖片: 0006R0_f02130.png\n",
            "預測完成，結果已保存至: /content/3/0006R0_f02130_mask.png\n",
            "處理圖片: 0001TP_010350.png\n",
            "預測完成，結果已保存至: /content/3/0001TP_010350_mask.png\n",
            "處理圖片: 0006R0_f01530.png\n",
            "預測完成，結果已保存至: /content/3/0006R0_f01530_mask.png\n",
            "處理圖片: 0006R0_f02460.png\n",
            "預測完成，結果已保存至: /content/3/0006R0_f02460_mask.png\n",
            "處理圖片: 0006R0_f00930.png\n",
            "預測完成，結果已保存至: /content/3/0006R0_f00930_mask.png\n",
            "處理圖片: 0006R0_f01800.png\n",
            "預測完成，結果已保存至: /content/3/0006R0_f01800_mask.png\n",
            "處理圖片: 0001TP_010320.png\n",
            "預測完成，結果已保存至: /content/3/0001TP_010320_mask.png\n",
            "處理圖片: 0006R0_f02730.png\n",
            "預測完成，結果已保存至: /content/3/0006R0_f02730_mask.png\n",
            "處理圖片: 0006R0_f02010.png\n",
            "預測完成，結果已保存至: /content/3/0006R0_f02010_mask.png\n",
            "處理圖片: 0006R0_f01830.png\n",
            "預測完成，結果已保存至: /content/3/0006R0_f01830_mask.png\n",
            "處理圖片: 0001TP_010020.png\n",
            "預測完成，結果已保存至: /content/3/0001TP_010020_mask.png\n",
            "處理圖片: 0001TP_009450.png\n",
            "預測完成，結果已保存至: /content/3/0001TP_009450_mask.png\n",
            "處理圖片: 0006R0_f01470.png\n",
            "預測完成，結果已保存至: /content/3/0006R0_f01470_mask.png\n",
            "處理圖片: 0006R0_f01860.png\n",
            "預測完成，結果已保存至: /content/3/0006R0_f01860_mask.png\n",
            "處理圖片: 0006R0_f02640.png\n",
            "預測完成，結果已保存至: /content/3/0006R0_f02640_mask.png\n",
            "處理圖片: 0006R0_f01230.png\n",
            "預測完成，結果已保存至: /content/3/0006R0_f01230_mask.png\n",
            "處理圖片: 0006R0_f01110.png\n",
            "預測完成，結果已保存至: /content/3/0006R0_f01110_mask.png\n",
            "處理圖片: 0001TP_009240.png\n",
            "預測完成，結果已保存至: /content/3/0001TP_009240_mask.png\n",
            "處理圖片: 0001TP_010110.png\n",
            "預測完成，結果已保存至: /content/3/0001TP_010110_mask.png\n",
            "處理圖片: 0001TP_009210.png\n",
            "預測完成，結果已保存至: /content/3/0001TP_009210_mask.png\n",
            "處理圖片: 0006R0_f01170.png\n",
            "預測完成，結果已保存至: /content/3/0006R0_f01170_mask.png\n",
            "處理圖片: 0006R0_f01140.png\n",
            "預測完成，結果已保存至: /content/3/0006R0_f01140_mask.png\n",
            "處理圖片: 0006R0_f01770.png\n",
            "預測完成，結果已保存至: /content/3/0006R0_f01770_mask.png\n",
            "處理圖片: 0001TP_010290.png\n",
            "預測完成，結果已保存至: /content/3/0001TP_010290_mask.png\n",
            "處理圖片: 0001TP_010140.png\n",
            "預測完成，結果已保存至: /content/3/0001TP_010140_mask.png\n",
            "處理圖片: 0001TP_009750.png\n",
            "預測完成，結果已保存至: /content/3/0001TP_009750_mask.png\n",
            "處理圖片: 0006R0_f02340.png\n",
            "預測完成，結果已保存至: /content/3/0006R0_f02340_mask.png\n",
            "處理圖片: 0001TP_010380.png\n",
            "預測完成，結果已保存至: /content/3/0001TP_010380_mask.png\n",
            "處理圖片: 0006R0_f02040.png\n",
            "預測完成，結果已保存至: /content/3/0006R0_f02040_mask.png\n",
            "處理圖片: 0001TP_009420.png\n",
            "預測完成，結果已保存至: /content/3/0001TP_009420_mask.png\n",
            "處理圖片: 0001TP_009810.png\n",
            "預測完成，結果已保存至: /content/3/0001TP_009810_mask.png\n",
            "處理圖片: 0001TP_009510.png\n",
            "預測完成，結果已保存至: /content/3/0001TP_009510_mask.png\n",
            "處理圖片: 0006R0_f02070.png\n",
            "預測完成，結果已保存至: /content/3/0006R0_f02070_mask.png\n",
            "處理圖片: 0006R0_f00960.png\n",
            "預測完成，結果已保存至: /content/3/0006R0_f00960_mask.png\n",
            "處理圖片: 0006R0_f01260.png\n",
            "預測完成，結果已保存至: /content/3/0006R0_f01260_mask.png\n",
            "處理圖片: 0001TP_009540.png\n",
            "預測完成，結果已保存至: /content/3/0001TP_009540_mask.png\n",
            "處理圖片: 0006R0_f01560.png\n",
            "預測完成，結果已保存至: /content/3/0006R0_f01560_mask.png\n",
            "處理圖片: 0006R0_f01200.png\n",
            "預測完成，結果已保存至: /content/3/0006R0_f01200_mask.png\n",
            "處理圖片: 0001TP_009480.png\n",
            "預測完成，結果已保存至: /content/3/0001TP_009480_mask.png\n",
            "處理圖片: 0006R0_f02160.png\n",
            "預測完成，結果已保存至: /content/3/0006R0_f02160_mask.png\n",
            "處理圖片: 0001TP_009840.png\n",
            "預測完成，結果已保存至: /content/3/0001TP_009840_mask.png\n",
            "處理圖片: 0006R0_f02430.png\n",
            "預測完成，結果已保存至: /content/3/0006R0_f02430_mask.png\n",
            "處理圖片: 0006R0_f02760.png\n",
            "預測完成，結果已保存至: /content/3/0006R0_f02760_mask.png\n",
            "處理圖片: 0006R0_f02100.png\n",
            "預測完成，結果已保存至: /content/3/0006R0_f02100_mask.png\n",
            "處理圖片: 0006R0_f02370.png\n",
            "預測完成，結果已保存至: /content/3/0006R0_f02370_mask.png\n",
            "處理圖片: 0006R0_f02610.png\n",
            "預測完成，結果已保存至: /content/3/0006R0_f02610_mask.png\n",
            "處理圖片: 0006R0_f02310.png\n",
            "預測完成，結果已保存至: /content/3/0006R0_f02310_mask.png\n",
            "處理圖片: 0001TP_009990.png\n",
            "預測完成，結果已保存至: /content/3/0001TP_009990_mask.png\n",
            "處理圖片: 0006R0_f02940.png\n",
            "預測完成，結果已保存至: /content/3/0006R0_f02940_mask.png\n",
            "處理圖片: 0006R0_f02670.png\n",
            "預測完成，結果已保存至: /content/3/0006R0_f02670_mask.png\n",
            "處理圖片: 0006R0_f01500.png\n",
            "預測完成，結果已保存至: /content/3/0006R0_f01500_mask.png\n",
            "處理圖片: 0001TP_009690.png\n",
            "預測完成，結果已保存至: /content/3/0001TP_009690_mask.png\n",
            "處理圖片: 0001TP_010080.png\n",
            "預測完成，結果已保存至: /content/3/0001TP_010080_mask.png\n",
            "處理圖片: 0006R0_f01710.png\n",
            "預測完成，結果已保存至: /content/3/0006R0_f01710_mask.png\n",
            "處理圖片: 0006R0_f01740.png\n",
            "預測完成，結果已保存至: /content/3/0006R0_f01740_mask.png\n",
            "處理圖片: 0006R0_f02910.png\n",
            "預測完成，結果已保存至: /content/3/0006R0_f02910_mask.png\n",
            "處理圖片: 0006R0_f01440.png\n",
            "預測完成，結果已保存至: /content/3/0006R0_f01440_mask.png\n",
            "處理圖片: 0006R0_f02400.png\n",
            "預測完成，結果已保存至: /content/3/0006R0_f02400_mask.png\n",
            "處理圖片: 0001TP_010050.png\n",
            "預測完成，結果已保存至: /content/3/0001TP_010050_mask.png\n",
            "處理圖片: 0001TP_009780.png\n",
            "預測完成，結果已保存至: /content/3/0001TP_009780_mask.png\n",
            "處理圖片: 0006R0_f02700.png\n",
            "預測完成，結果已保存至: /content/3/0006R0_f02700_mask.png\n",
            "處理圖片: 0001TP_009390.png\n",
            "預測完成，結果已保存至: /content/3/0001TP_009390_mask.png\n",
            "處理圖片: 0006R0_f01410.png\n",
            "預測完成，結果已保存至: /content/3/0006R0_f01410_mask.png\n",
            "處理圖片: 0001TP_009720.png\n",
            "預測完成，結果已保存至: /content/3/0001TP_009720_mask.png\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torchvision.transforms as transforms\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "from torchvision.utils import save_image\n",
        "import os\n",
        "\n",
        "class UNetPredictor:\n",
        "    def __init__(self, model_path, device=None):\n",
        "        self.device = device if device else torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "        # 載入模型架構\n",
        "        self.model = UNet(in_channels=3, out_channels=1)\n",
        "\n",
        "        # 載入訓練好的權重\n",
        "        self.model.load_state_dict(torch.load(model_path, map_location=self.device))\n",
        "        self.model.to(self.device)\n",
        "        self.model.eval()\n",
        "\n",
        "        # 設定圖片預處理\n",
        "        self.transform = transforms.Compose([\n",
        "            transforms.Resize((256, 256)),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                               std=[0.229, 0.224, 0.225])\n",
        "        ])\n",
        "\n",
        "    def predict(self, image_path, output_path):\n",
        "        \"\"\"\n",
        "        預測單張圖片的遮罩\n",
        "\n",
        "        Args:\n",
        "            image_path (str): 輸入圖片的路徑\n",
        "            output_path (str): 輸出遮罩的保存路徑\n",
        "        \"\"\"\n",
        "        try:\n",
        "            # 載入並預處理圖片\n",
        "            image = Image.open(image_path).convert('RGB')\n",
        "            original_size = image.size\n",
        "\n",
        "            # 轉換圖片格式\n",
        "            image_tensor = self.transform(image).unsqueeze(0)\n",
        "            image_tensor = image_tensor.to(self.device)\n",
        "\n",
        "            # 進行預測\n",
        "            with torch.no_grad():\n",
        "                output = self.model(image_tensor)\n",
        "                prediction = torch.sigmoid(output)\n",
        "\n",
        "            # 將預測結果轉換為二值遮罩 (threshold = 0.5)\n",
        "            binary_mask = (prediction > 0.5).float()\n",
        "\n",
        "            # 保存預測結果\n",
        "            save_image(binary_mask, output_path)\n",
        "\n",
        "            print(f\"預測完成，結果已保存至: {output_path}\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"預測過程發生錯誤: {str(e)}\")\n",
        "            raise\n",
        "\n",
        "def predict_batch(model_path, input_dir, output_dir):\n",
        "    \"\"\"\n",
        "    批次預測多張圖片\n",
        "\n",
        "    Args:\n",
        "        model_path (str): 模型檔案路徑\n",
        "        input_dir (str): 輸入圖片目錄\n",
        "        output_dir (str): 輸出遮罩目錄\n",
        "    \"\"\"\n",
        "    # 創建輸出目錄\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "    # 初始化預測器\n",
        "    predictor = UNetPredictor(model_path)\n",
        "\n",
        "    # 獲取所有圖片檔案\n",
        "    image_files = [f for f in os.listdir(input_dir) if f.endswith(('.png', '.jpg', '.jpeg'))]\n",
        "\n",
        "    print(f\"開始處理 {len(image_files)} 張圖片...\")\n",
        "\n",
        "    # 對每張圖片進行預測\n",
        "    for image_file in image_files:\n",
        "        input_path = os.path.join(input_dir, image_file)\n",
        "        output_path = os.path.join(output_dir, f\"{os.path.splitext(image_file)[0]}_mask.png\")\n",
        "\n",
        "        print(f\"處理圖片: {image_file}\")\n",
        "        predictor.predict(input_path, output_path)\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    # 使用範例\n",
        "    model_path = '/content/unet_model.pth'  # 訓練好的模型路徑\n",
        "    input_dir = '/content/1'      # 測試圖片目錄\n",
        "    output_dir = '/content/3'      # 預測結果輸出目錄\n",
        "\n",
        "    predict_batch(model_path, input_dir, output_dir)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchvision.transforms as transforms\n",
        "import cv2\n",
        "import numpy as np\n",
        "import os\n",
        "from PIL import Image\n",
        "\n",
        "class UNetPredictor:\n",
        "    def __init__(self, model_path, device=None):\n",
        "        \"\"\"\n",
        "        初始化 UNet 預測器，載入模型權重和設置設備\n",
        "        \"\"\"\n",
        "        self.device = device if device else torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "        # 載入 U-Net 模型架構\n",
        "        self.model = UNet(in_channels=3, out_channels=1)\n",
        "\n",
        "        # 載入訓練好的模型權重\n",
        "        self.model.load_state_dict(torch.load(model_path, map_location=self.device))\n",
        "        self.model.to(self.device)\n",
        "        self.model.eval()\n",
        "\n",
        "        # 設定圖片預處理方式\n",
        "        self.transform = transforms.Compose([\n",
        "            transforms.Resize((256, 256)),  # 調整圖片大小\n",
        "            transforms.ToTensor(),  # 轉換為 Tensor\n",
        "            transforms.Normalize(mean=[0.485, 0.456, 0.406],  # 圖片標準化\n",
        "                                 std=[0.229, 0.224, 0.225])\n",
        "        ])\n",
        "\n",
        "    def predict_frame(self, frame):\n",
        "        \"\"\"\n",
        "        預測單一幀的遮罩\n",
        "        frame: 單幀影像\n",
        "        return: 預測的二值遮罩 (0 或 1)\n",
        "        \"\"\"\n",
        "        # 將 BGR 圖片轉換為 RGB 格式\n",
        "        image = Image.fromarray(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n",
        "\n",
        "        # 預處理並轉換為 tensor\n",
        "        image_tensor = self.transform(image).unsqueeze(0).to(self.device)\n",
        "\n",
        "        # 不進行梯度計算，預測結果\n",
        "        with torch.no_grad():\n",
        "            output = self.model(image_tensor)\n",
        "            prediction = torch.sigmoid(output)  # 使用 sigmoid 來獲得概率值\n",
        "            binary_mask = (prediction > 0.5).float().cpu().numpy().squeeze()  # 轉換為二值遮罩 (0 或 1)\n",
        "\n",
        "        return binary_mask  # 這裡是 0 (黑色) 或 1 (白色) 的二值遮罩\n",
        "\n",
        "def process_video(model_path, video_path, final_mask_path, frame_step=15, black_threshold=3):\n",
        "    \"\"\"\n",
        "    讀取影片，每 `frame_step` 幀預測一次，並根據每個像素的黑色次數決定最終遮罩\n",
        "    model_path: 訓練好的模型路徑\n",
        "    video_path: 影片路徑\n",
        "    final_mask_path: 最終遮罩輸出路徑\n",
        "    frame_step: 每幾幀處理一次\n",
        "    black_threshold: 像素點必須在多少幀中為黑色，最終遮罩才變黑\n",
        "    \"\"\"\n",
        "    predictor = UNetPredictor(model_path)  # 初始化預測器\n",
        "    cap = cv2.VideoCapture(video_path)  # 讀取影片\n",
        "\n",
        "    if not cap.isOpened():\n",
        "        print(\"無法開啟影片\")\n",
        "        return\n",
        "\n",
        "    fps = int(cap.get(cv2.CAP_PROP_FPS))  # 取得影片 FPS\n",
        "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))  # 總幀數\n",
        "    height, width = None, None\n",
        "    frame_idx = 0\n",
        "\n",
        "    black_pixel_count = None  # 記錄每個像素變黑的次數\n",
        "    frame_count = 0  # 計算總共處理的幀數\n",
        "\n",
        "    while True:\n",
        "        ret, frame = cap.read()\n",
        "        if not ret:\n",
        "            break  # 影片播放結束\n",
        "\n",
        "        if height is None or width is None:\n",
        "            height, width, _ = frame.shape\n",
        "            black_pixel_count = np.zeros((height, width), dtype=np.uint16)  # 初始化黑色像素計數\n",
        "\n",
        "        # **每 `frame_step` 幀處理一次**\n",
        "        if frame_idx % frame_step == 0:\n",
        "            mask = predictor.predict_frame(frame)  # 取得預測遮罩 (0 或 1)\n",
        "            mask = cv2.resize(mask, (width, height))  # 調整遮罩大小回原解析度\n",
        "            mask = (mask * 255).astype(np.uint8)  # 轉換為 0 (黑) 或 255 (白)\n",
        "\n",
        "            # 更新黑色像素出現次數\n",
        "            black_pixel_count[mask == 0] += 1\n",
        "\n",
        "            frame_count += 1  # 記錄處理的幀數\n",
        "\n",
        "        frame_idx += 1\n",
        "\n",
        "    cap.release()  # 釋放影片資源\n",
        "\n",
        "    # **根據黑色像素的出現次數決定最終遮罩**\n",
        "    final_mask = np.ones((height, width), dtype=np.uint8) * 255  # 預設為全白\n",
        "    final_mask[black_pixel_count >= black_threshold] = 0  # 若某像素在多於 `black_threshold` 幀中為黑色，最終該像素為黑色\n",
        "\n",
        "    # 儲存最終遮罩\n",
        "    cv2.imwrite(final_mask_path, final_mask)\n",
        "    print(f\"最終遮罩已儲存: {final_mask_path}, 影片共使用 {frame_count} 張影格進行處理\")\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    # 設定模型路徑、影片路徑和最終遮罩的輸出路徑\n",
        "    model_path = '/content/unet_model.pth'\n",
        "    video_path = '/content/下載.mp4'\n",
        "    final_mask_path = '/content/final_mask.png'  # 儲存最終的遮罩圖片\n",
        "    frame_step = 5  # 每 5 幀預測一次\n",
        "    black_threshold = 4  # 若某像素在 4 幀中變黑，最終結果才變黑\n",
        "\n",
        "    process_video(model_path, video_path, final_mask_path, frame_step, black_threshold)\n"
      ],
      "metadata": {
        "id": "vSLZfSheLKRs"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}